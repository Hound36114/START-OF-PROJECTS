# -*- coding: utf-8 -*-
"""Text_classification_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AiIuyWvhdaF7FsK7riZEisqDWousE3K0
"""

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

train_data, validation_data, test_data = tfds.load(name = 'imdb_reviews',
                                                  split = ('train[:60%]', 'train[60%:]', 'test'),
                                                  as_supervised = True)    
def test_dataset_not_empty():
    example = next(iter(train_data))
    assert example is not None, "Training dataset is empty!"

test_dataset_not_empty()

def test_label_values():
    batch, label = next(iter(train_data.batch(10)))
    assert len(batch) > 0, "Batch is empty!"
    assert len(label) > 0, "Labels are empty!"

test_label_values()

train_example_batch, train_labels_batch = next(iter(train_data.batch(10)))

train_example_batch

train_labels_batch

class HubWrapper(tf.keras.layers.Layer):
  #Wraps a TensorFlowâ€‘Hub SavedModel so Keras3 sees it as a regular Layer.

  def __init__(self, url: str, trainable: bool = False, **kwargs):
    super().__init__(trainable = trainable, **kwargs)
    self.url = url
    self._hub_model = None #lazy-load in build()

  def build(self, input_shape):
    self._hub_model = hub.load(self.url)
    self._hub_model = tf.function(self._hub_model)
    super().build(input_shape)

  def call(self, inputs):
    outputs = self._hub_model(inputs)

    if isinstance(outputs, dict):
      outputs = outputs.get("default", list(outputs.values())[0])

    return outputs

embedding = "https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1"
hub_layer = HubWrapper(embedding, dtype = tf.string, trainable = True, name = "text_embedding")

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation = 'relu'))
model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))

model.summary()

def test_model_output_range():
    pred = model(["Good movie", "Bad movie"])   # small test
    for p in pred.numpy().flatten():
        assert 0 <= p <= 1

test_model_output_range()

model.compile(optimizer = 'adam',
              loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),
              metrics = ['accuracy'])

history = model.fit(train_data.shuffle(10000).batch(100),
                    epochs = 25,
                    validation_data = validation_data.batch(100),
                    verbose = 1)

results = model.evaluate(test_data.batch(100), verbose = 2)

for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))

import matplotlib.pyplot as plt
def plot_class_distribution():
    pos_count = 0
    neg_count = 0
    for _, label in train_data.take(1000):  # check first 1000 reviews
        if label.numpy() == 1:
            pos_count += 1
        else:
            neg_count += 1

    plt.bar(['Positive', 'Negative'], [pos_count, neg_count], color=['green', 'red'])
    plt.title("Class Distribution in Sample of 1000 Reviews")
    plt.ylabel("Number of Reviews")
    plt.show()



plot_class_distribution()

